---
title: 'Statistics 305/605: Introduction to Biostatistical Methods for Health Sciences'
subtitle: 'Chapter 6: Probability'
author: "Jinko Graham"
output: 
  beamer_presentation:
    includes:
      in_header: ../header_pagenum.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Basics (Section 6.1)

## Definitions

* A phenomenon is _random_ if individual outcomes
are uncertain but there is a predictable behaviour in a 
large number of repetitions 
(random $\not=$ haphazard).
* The _probability_ of any outcome is the proportion of 
times the outcome would occur in a very long (infinitely long)
series of repetitions.
* Repetitions, or _trials_, are said to be 
_independent_ if the outcome of one trial does not affect the 
outcome of another 
(e.g., coin tosses).

## Probability Models

Three parts:

1. the sample space ${\cal S}$,
2. a list ${\cal E}$ of all possible events $E$, and
3. a way of assigning probabilities to events.

## Sample Space, ${\cal S}$

The set of all possible outcomes.

* *Example:* Two coin tosses. 
$${\cal S} = \{ HH, HT, TH, TT \}$$
\scriptsize 
(the braces $\{$ and $\}$ denote a set in between)
\normalsize

* *Example:* Roll of two dice. 
$${\cal S} = \{ 11, 12, 13, \ldots, 56, 66 \} $$

* *Example:* 
Estrogen $+$ progestin (EP) or placebo ($\overline{\rm EP}$)
and breast cancer (BC) or not ($\overline{\rm BC}$). 
$${\cal S} = \{ \mbox{EP\&BC}, \;\; 
\mbox{EP\&$\overline{\rm BC}$}, \;\;
\mbox{$\overline{\rm EP}\&{\rm BC}$}, \;\; 
\mbox{$\overline{\rm EP}$\&$\overline{\rm BC}$}
 \} $$


## List of All Possible Events, ${\cal E}$

* *Example:* Two coin tosses.
\begin{eqnarray*}
E_1& = &\{ HH \} = \{ \mbox{2 heads} \} \\
E_2& = &\{ HT, TH \} = \{ \mbox{1 head, 1 tail}\} \\
E_3& = &\{ HT, TH, TT \} = \{ \mbox{at least 1 tail} \} \\
E_4& = &\{ HT, TH, HH \} = \{ \mbox{at least 1 head} \}\\
E_5&=& \{ HH, HT \} = \{ \mbox{head on 1st toss} \} \\
E_6&=& \{ HH, TH \} = \{ \mbox{head on 2nd toss} \} \\
E_7&=& \{ TH \} \\
& & \mbox{etc.} \\
& & \\
{\cal E} & = & \{ E_1, E_2, E_3, \ldots \}\\
\end{eqnarray*}

## 

* *Example:* Roll of two dice.
\begin{eqnarray*}
E_1& = &\{ 11, 12, 21 \} = \{ \mbox{sum $\leq$ 3} \} \\
E_2& = &\{ 11, 13, \ldots, 55, 66 \} = \{ \mbox{sum is even}\} \\
& & \mbox{etc.} \\
& & \\
{\cal E} & = & \{ E_1, E_2, \ldots \}
\end{eqnarray*}

## Events, cont.

* *Example:*  Estrogen $+$ progestin (EP) or not ($\overline{\rm EP}$)
and breast cancer (BC) or not ($\overline{\rm BC}$).
\begin{eqnarray*}
E_1 &= & \{ \mbox{EP\&BC}, \; \; \mbox{EP\&$\overline{\rm BC}$} \} = 
\{ \mbox{EP} \} \\
E_2 &= & \{  \mbox{EP\&BC}, \; \; \mbox{$\overline{\rm EP}$\&BC} \} =
\{ \mbox{BC} \} \\
E_3 &= & \{ \mbox{$\overline{\rm EP}$\&BC}, \;\;
\mbox{$\overline{\rm EP}$\&$\overline{\rm BC}$}
\} = 
\{ \mbox{$\overline{\rm EP}$} \} \\
E_4 &= &  \mbox{EP\&BC}  \\
E_5 &= & \mbox{EP\&$\overline{\rm BC}$} \\
& & \mbox{etc.} \\
& & \\
{\cal E} & = & \{ E_1, E_2, E_3,\ldots \}
\end{eqnarray*}

## Operations on Events

* The _intersection_ of two events $A$ and $B$ is the collection of outcomes
that are in both $A$ and $B$:
    + Denoted $A \cap B$ and read as ``$A$ *and* $B$''.
* The _union_ of two events $A$ and $B$ is the collection of outcomes that
are in either $A$ or $B$ (or both):
    + Denoted $A \cup B$ and read as "$A$ *or* $B$".
* The _complement_ of an event $A$ is the collection of outcomes not in $A$:
    + Denoted $\overline{A}$ and read as "not $A$".

## Probabilities of Events: Four Rules


A probability model tells us the probability of each possible
event. Notation: $P(E)$ for ``probability of event $E$''. 

\medskip

Basic rules of probability:

1. For any event $E$, $0 \leq P(E) \leq 1$.
2. $P({\cal S}) = 1$ (something must happen)
3. For an event $E$, $P(\overline{E}) = 1 - P(E)$.
4. Two events are _disjoint_ if they have no 
outcomes in common. 
    + If $A$ and $B$ are disjoint, then the _addition rule_ holds:
$P(A \mbox{ or } B) = P(A \cup B)
= P(A) + P(B)$ 




## Venn Diagrams

A way to picture events and probabilities.

\medskip

\begin{center}
\resizebox{2.2in}{!}{
\begin{picture}(200,200)
\put(0,0){\line(0,1){200}}
\put(0,200){\line(1,0){200}}
\put(0,0){\line(1,0){200}}
\put(200,0){\line(0,1){200}}
\put(60,150){\oval(100,60)}
\put(40,150){$E$}
\put(140,50){$\overline{E}$}
\put(205,170){$\cal{S}$}
\put(-40,210){The event $E$ and its complement $\overline{E}$
within the sample space ${\cal S}$.}
\end{picture}
}
\end{center}
The area of an event represents its probability; e.g., we can see
$P(E) < P(\overline{E})$.

## Venn Diagrams, cont.

\resizebox{2.0in}{!}{
\begin{picture}(200,200)
\put(0,0){\line(0,1){200}}
\put(0,200){\line(1,0){200}}
\put(0,0){\line(1,0){200}}
\put(200,0){\line(0,1){200}}
\put(60,150){\oval(100,60)}
\put(40,150){$A$}
\put(140,60){\oval(100,60)}
\put(140,50){$B$}
\put(20,220){Disjoint sets $A$ and $B$}
\end{picture} 
}
\resizebox{2.0in}{!}{
\begin{picture}(200,200)
\put(0,0){\line(0,1){200}}
\put(0,200){\line(1,0){200}}
\put(0,0){\line(1,0){200}}
\put(200,0){\line(0,1){200}}
\put(80,110){\oval(100,60)}
\put(40,100){$A$}
\put(120,80){\oval(100,60)}
\put(150,70){$B$}
\put(70,90){$A\cap B$}
\put(10,220){Non-disjoint sets with intersection $A\cap B$.}
\end{picture} 
}


## The General Addition Rule

General addition rule: $P(A \cup B) = P(A) + P(B) - P(A\cap B)$:

\begin{center}
\resizebox{2.5in}{!}{
\begin{picture}(200,200)
\put(0,0){\line(0,1){200}}
\put(0,200){\line(1,0){200}}
\put(0,0){\line(1,0){200}}
\put(200,0){\line(0,1){200}}
\put(80,110){\oval(100,60)}
\put(40,100){$A$}
\put(120,80){\oval(100,60)}
\put(150,70){$B$}
\put(70,90){$A\cap B$}
\end{picture}
}
\end{center}
* Adjusts for double-counting the intersection.

## Independent events

* Events $A$ and $B$ are _independent_ if 
knowing that one has occurred provides no information 
about whether the other will.

* E.G. Consider a random experiment involving 2 tosses of a fair coin
    + Sample space is ${\cal S}=\{HH, HT, TH, TT \}$
    + All 4 outcomes in ${\cal S}$ have chance $1/4$.

* Knowing $H$ occurs on the 1st toss provides no info about whether $H$ will occur in the 2nd toss. 

* Let $A = \{ HH, HT \}$ be the event of a head on 
    the 1st toss and $B=\{ HH, TH \}$ be the event of an head on the 2nd toss. 
    + The events $A$ and $B$ are independent.


## Multiplication rule for independent events

* Multiplication rule for independent events:
    + If $A$ and $B$ are independent, then 
$P(A\cap B)=P(A)\times P(B)$.

* In the coin-toss experiment, $A = \{ HH, HT \}$ 
  and $B=\{ HH, TH \}$. 
So $A \cap B = \{ HH \}$.

* We have that $P(A)=P(\{HH, HT\})=P(HH)+P(HT)=1/4+1/4=1/2$ and  $P(B)=P(\{HH, TH\})=P(HH)+P(TH)=1/4+1/4=1/2$.
* So, $P(A \cap B) = P(A)\times P(B) = \frac{1}{2} \times \frac{1}{2}=1/4=P(HH)$.
    
# Conditional Probability (Section 6.2)

## General Multiplication Rule

* For 2 (not necessarily independent) events, the rule is: 
\begin{eqnarray*}
P(A \cap B) & = & P(A \mid B) \times P(B), \; {\rm or} \\
P(A \cap B) & = & P(B \mid A) \times P(A)
\end{eqnarray*}
* Read $A\mid B$ as "$A$ given $B$" and $P( A \mid B)$ as
"the conditional probability of $A$ given $B$."
* When $A$ and $B$ are independent, 
    + $P(A \mid B) =P(A)$ and $P(B \mid A) = P(B)$, so that 
    $$P(A \cap B) = P(A) \times P(B).$$ 
* The multiplication rule for independent events is thus a special
case of the general multiplication rule.


## Conditional Probability

* Knowing $B$ has happened may modify the probability of $A$; 
    + i.e., the _conditional probability_ of $A$ given $B$ 
    may not be the probability of $A$; or $P(A \mid B) \not= P(A)$.

* The general multiplication rule 
follows from the definition of conditional probabilities:
\begin{eqnarray*}
P(A \mid B) & = & P(A \cap B)/P(B)  \quad \mbox{\scriptsize{normalizing by size of $B$}}\\ 
P(B \mid A) & = & P(A \cap B)/P(A)
\end{eqnarray*}
* E.G., for $P(B\mid A)$, we are restricting our sample space 
to the outcomes in $A$ and considering those that are also in $B$. 
    + the probability is therefore the proportion of outcomes \underline{in $A$ that are also in $B$}.

## Conditional Probability Picture

* On a Venn diagram, $P(B|A)$ is the 
proportion of the area of $A$ occupied by $\{A\cap B\}$:

\resizebox{2.5in}{!}{
\begin{picture}(200,200)
\put(0,0){\line(0,1){200}}
\put(0,200){\line(1,0){200}}
\put(0,0){\line(1,0){200}}
\put(200,0){\line(0,1){200}}
% A-complement box
\put(160,0){\line(1,0){40}}
\put(160,40){\line(1,0){40}}
\put(160,0){\line(0,1){40}}
\put(200,0){\line(0,1){40}}
{\color{red} \put(160,20){\oval(40,20)}}
\put(40,150){$\overline{A}$}
\put(190,25){$A$}
{\color{red} \put(150,20){$B$}}
\end{picture}
}
\resizebox{1.6in}{!}{
\begin{picture}(200,200)
\put(20,140){$P(B\cap A)= P(B \cap \overline{A})$.}
\put(20,120){Now $P(B\mid A)=P(B\cap A)/P(A)$}
\put(20,100){and $P(B \mid \overline{A})=P(B\cap \overline{A})/P(\overline{A}).$}
\put(20,80){Numerators same but denominators different;}
\put(20,60){Since $P(A) < P(\overline{A})$, dividing by smaller number}
\put(20,40){for $P(B \mid A)$ than for $P(B\cap \overline{A})$.}
\put(20,20){So $P(B\mid A) > P(B \mid \overline{A})$ as visualized on diagram.}
\end{picture}
}

# Bayes' Theorem (Section 6.3)

## Bayes' Theorem and Partitioning the Sample Space

* Connects conditional probabilities:
$$
P(A \mid B ) = \frac{P(B \mid A) P(A)}{P(B)}.
$$
* Writing $B$ as $\{ A\cap B \} \cup \{\bar{A} \cap B \}$ and
noticing that these two sets are disjoint (see previous diagram) means that we can write 
$$P(B) = P(A\cap B) + P(\bar{A} \cap B).$$
* Applying the general multiplication rule to both terms in the sum for $P(B)$, we obtain
$$P(B) = P(B \mid A) P(A) + P(B \mid \bar{A}) P(\bar{A}).$$
* Which means an alternate form of Bayes' Theorem is
$$ P(A \mid B ) = \frac{P(B \mid A) P(A) }{P(B \mid A) P(A) + P(B \mid \bar{A}) P(\bar{A})}. $$

* See text, pg 134, for an extension to partitioning of the sample space
into more than two ($A$ and $\bar{A}$) events.


## Application of Bayes' Theorem: Diagnostic Testing

* Let $A$=\{disease\}, $B$=\{test positive\},
* Suppose $P(B\mid A)=0.99$, $P(B\mid \bar{A})=1/1000$; 
a good diagnostic test. Suppose rare disease with $P(A) = 1/1000000$. 
* What is $P(A \mid B)$, the probability you have the disease given that you tested positive?
\begin{eqnarray*}
P(A\mid B) & = & 
\frac{P(B\mid A) P(A)}{P(B\mid A)P(A) + P(B\mid \overline{A})P(\overline{A})} \\
&= &  \frac{0.99(1/1000000)}{0.99(1/1000000) + (1/1000)(0.999999)}  \\
& \approx & 1/1000 ! \; \;\; \mbox{very low despite a good test}
\end{eqnarray*}

## Diagnostic Testing Picture

The following picture is not to scale, but gives the idea:

\resizebox{2in}{!}{
\begin{picture}(200,200)
\put(0,0){\line(0,1){200}}
\put(0,200){\line(1,0){200}}
\put(0,0){\line(1,0){200}}
\put(200,0){\line(0,1){200}}
\put(180,0){\line(1,0){20}} 
\put(180,20){\line(1,0){20}}
\put(180,0){\line(0,1){20}}
\put(200,0){\line(0,1){20}}
{\color{red} \put(100,10){\oval(199,16)}}
\put(40,150){$\overline{A}$}
\put(185,7){$A$}
{\color{red} \put(100,8){$B$}}
\put(205,180){E.G. A$=$disease, B$=$test $+$}
\end{picture}
}

* $P(B\mid A)$ may be nearly one, but $P(A\mid B)$ can still
be quite small if $A$ is rare.
* More on diagnostic testing in the next section.

# Diagnostic Testing (Section 6.4)

## Diagnostic Testing

* Diagnostic tests are used to screen for diseases.
    + E.G., pap smear to screen for cervical cancer
* The test can be positive ($T^+$) or negative ($T^-$), and an individual
may have the disease ($D^+$) or not ($D^-$); 
    + e.g., for $n$ individuals:
\begin{center}
\begin{tabular}{ll|c|c|r}
& \multicolumn{1}{c}{}& \multicolumn{2}{c}{Disease Status} & \\
&\multicolumn{1}{c}{} & \multicolumn{1}{c}{$D^+$} & \multicolumn{1}{c}{$D^-$} & \\ \cline{3-4}
Test & $T^+$ & True & False & TP+FP \\
Result & & Positive (TP) & Positive (FP) & \\ \cline{3-4}
& $T^-$ &  False & True & FN+TN \\
& & Negative (FN) & Negative (TN) \\ \cline{3-4}
&\multicolumn{1}{c}{} & \multicolumn{1}{c}{TP+FN} & \multicolumn{1}{c}{FP+TN} & $n$ \\
\end{tabular}
\end{center}

## Diagnostic Test Performance

* The utility of a diagnostic test can be described in terms of 
    + the probability of _test outcomes_ given true disease status
    (sensitivity and specificity), or
    + the probability of _disease status_ given test outcomes
    (positive and negative predictive value). 

## Sensitivity of a Test

* Let's focus on the diseased ($D+$): they can either test positive or negative.
    + Those who are diseased and test positive are truly positive (TP)
    + Those who are diseased and test negative are falsely negative (FN)
* The observed true-positive rate
is the proportion of diseased who are truly positive,
or TP/(TP+FN). 
    + TP/(TP+FN) is an estimate of $P(T^+ \mid D^+)$, known as the _sensitivity_  of the test. 
    + A test that has sensitivity near one is good.
   
   
* The complement, FN/(TP+FN) is an estimate of the _false_-negative rate, 
$P(T^- \mid D^+)$.

## Specifity of a Test

* Now let's focus on the non-diseased: they can either test positive or negative.
    + Those who are non-diseased and test positive are falsely positive (FP)
    + Those who are non-diseased and test negative are truly negative (TN)
    
* The observed true-negative rate is the proportion of non-diseased ($D^-$) who are
truly negative (TN), or TN/(TN+FP).
    + TN/(TN+FP) is an estimate of the true-negative rate $P(T^- \mid D^-)$,
known as the _specificity_ of the test. 
    + Specificity near one is good.
    
* The complement, FP/(TN+FP), is an estimate of the 
false-positive rate $P(T^+ \mid D^-)$.

* We can estimate the sensitivity and specificity of a diagnostic
test from samples of diseased and non-diseased subjects who receive the test. 

## Positive Predictive Value of a Test

* Let's focus on those who test positive ($T+$): they can either be diseased or non-diseased.
    + Those who test positive and are diseased are truly positive (TP)
    + Those who test positive and are non-diseased are falsely positive (FP)
    
* The observed proportion of those who test positive that are truly 
diseased is TP/(TP+FP).
    + TP/(TP+FP) is an estimate of $P(D^+ \mid T^+)$, the 
true-discovery rate or _positive predictive value_ of the test (aka _precision_).
    + The complement, FP/(TP+FP), is an estimate of the _false-discovery rate_ of the test, 
$P(D^- \mid T^+)$.

## Negative Predictive Value of a Test

* Finally, let's focus on those who test negative ($T^-$): they can either be diseased or non-diseased.
    + Those who test negative and are diseased are falsely negative (FN)
    + Those who test negative and are non-diseased are truly negative (TN)

* The observed proportion of those who test negative that are  non-diseased is TN/(TN+FN).
    + TN/(TN+FN) is an estimate of $P(D^- \mid T^-)$, the 
_negative predictive value_ of the test.
    + The complement, FN/(TN+FN), that estimates 
    $P(D^+ \mid T^-)$ is unnamed. 
* For rare diseases, we may estimate the
positive and negative predictive values
using Bayes' Theorem and
the population prevalence of disease, $P(D^+)$.

## Predictive Values by Bayes' Theorem: Motivation

* When the disease we're testing for is rare, we expect to have very few diseased individuals in our random sample.

* Unfortunately, samples with few diseased persons yield unreliable estimates of
    +  $P(D^+\mid T^+)$, the positive predictive value, and
    +  $P(D^- \mid T^-)$, the negative predictive value ($=1-P(D^+ \mid T^-)$).

* Fortunately, we can use Bayes' Theorem to express the
positive- and negative-predictive values
in terms of:
    + the sensitivity, $P(T^+ \mid D^+)$; 
    + the specificity, $P(T^-\mid D^-)$; and
    + the disease prevalence, $P(D^+)$.
    
* The sensitivity, specificity and disease prevalence can
be reliably estimated if a population registry of cases (diseased persons) and census information is available. 
    + E.G. BC Cancer Agency's Registry and the Canadian Census
    

##  Population Registries and the Census are Crucial for Informed Policy Decisions

* A decent-sized 
sample of diseased individuals can be obtained from a population disease registry, which allows us to estimate
the sensitivity of a diagnostic test, $P(T^+ \mid D^+)$.

* A large sample of non-diseased individuals allows reliable
estimation of the specificity of a test, $P(T^-\mid D^-)$.
* The population registry and census data also provide the
numbers with the disease and in the population, 
respectively, which lead to an
estimate of the disease prevalence, $P(D^+)$.

* Note: Other approaches to estimating the prevalence will 
not be covered, but see Section 6.4.4 of the text if you are interested.

* Raises an interesting point: Governments that care about making informed decisions should think twice before eliminating public resources such as disease registries and the census. 



## Predictive Values by Bayes' Theorem: Details

* The positive predictive value is
\begin{eqnarray*}
P(D^+ \mid T^+) &=& 
\frac{P(T^+ \mid D^+) P(D^+)}{P(T^+ \mid D^+) P(D^+) + P(T^+ \mid D^-) P(D^-)} \\
& = &
\frac{\mbox{sens}\times  P(D^+)}{\mbox{sens} \times P(D^+) + (1-\mbox{spec})\times (1-P(D^+))}
\end{eqnarray*}

* The negative predictive value is 
\begin{eqnarray*}
P(D^- \mid T^-) &=& 
\frac{P(T^- \mid D^-)P(D^-)}{P(T^- \mid D^-)P(D^-) + P(T^- \mid D^+)P(D^+)} \\
& = &
\frac{\mbox{spec}\times (1-P(D^+))}{\mbox{spec} \times (1-P(D^+)) + (1-\mbox{sens})\times P(D^+)}
\end{eqnarray*}

## Positive Predictive Value Example

* Pap smear performance as a diagnostic test for cervical cancer
(see the text, pgs 136-137).
* Estimated sensitivity of the test is 0.8375
* Estimated specificity of the test is 0.8136
* Estimated prevalence of cervical cancer is 8.3 per 100,000 or 
0.000083.
* Hence the positive predictive value is

\small
\begin{eqnarray*}
P(D^+ \mid T^+) &=& 
\frac{\mbox{sens}\times  P(D^+)}{\mbox{sens} \times P(D^+) + (1-\mbox{spec})\times (1-P(D^+))} \\
& = &
\frac{0.8375 \times 0.000083}{0.8375 \times 0.000083 + (1-0.8136)\times (1-0.000083)} \\
& = & 0.000373
\end{eqnarray*}
or only 37.3 expected to actually have cervical cancer per 100,000 positive tests.


# The Relative Risk and Odds Ratio (Section 6.5)

## The Relative Risk

* Example: Organochlorines (OGC) are used in farm pesticides. Does exposure to OGC increase the risk of 
developing non-Hodgkin lymphoma (NHL)?

\begin{center}
\includegraphics[height=1in]{Spinelli07Clip.pdf}
\end{center}

* Compare the risk of NHL in those exposed to OGC to the risk in those unexposed to OGC.
* The multiplicative factor by which OGC exposure increases the risk
of NHL is the relative risk, or $RR$:
$$
RR = \frac{P(\mbox{disease} \mid \mbox{exposed})}
{P(\mbox{disease} \mid \mbox{unexposed})}
$$

## Estimating the Relative Risk

* If we obtain a sample of exposed individuals, we can estimate 
$P(\mbox{disease} \mid \mbox{exposed})$ by the proportion of diseased individuals
in the sample.
* Similarly, if we obtain a sample of unexposed individuals, we can estimate
$P(\mbox{disease} \mid \mbox{unexposed})$ by the proportion of diseased individuals
in the sample.
* The ratio of our estimates is then an estimate of the $RR$.

## Rare Diseases

* For rare diseases it may be difficult to collect enough data to 
estimate the $RR$
    + E.G., about 20 cases of NHL per 100,000 people per year
(about 1 per 5,000 per year).
    + With few disease cases in either exposed or unexposed groups,
estimates of $P(\mbox{disease} \mid \mbox{exposed})$ and
$P(\mbox{disease} \mid \mbox{unexposed})$ will be unreliable.
    + This is where population-based disease registries come in. Disease registries are an ample source of cases.

* The BC Cancer Agency study of NHL used a cost-effective study design called
the case-control design and the BC Cancer Agency's provincial registry of NHL cases.  

* The classic example of a case-control study: 
Doll and Hill's smoking and lung cancer study.


## Doll and Hill's Study

* Research Question: Is smoking associated with lung cancer?
    + (We know now, yes, but in the 1950's this was a controversial hypothesis.)
* Data on hospitalized patients from about 20 hospitals around
London between Apr. 1948 - Feb. 1952. 
* Hospital staff contact investigators whenever a new patient
is admitted for lung cancer (case) and also interview patient about 
smoking habits (exposure).
* Also interview a patient admitted for something other than lung
cancer (controls) at the same hospital.

## Doll and Hill's Data

\begin{center}
\begin{tabular}{llcc} 
 & &  case & control  \\ \hline
Smoke & Yes & 1350 & 1296  \\
 (E)  & No  & 7 & 61  \\ \hline
 & & 1357 & 1357
\end{tabular}
\end{center}

* Problem: Proportions of cases in the smokers or non-smokers
do not reflect population proportions because of the sampling 
design, which over-samples cases relative to their frequency in the population. 
    + In the table above, half our sample is cases and half controls. But in the population, a small proportion is cases and a large proportion is controls (because lung cancer is rare).
    
* **Can't estimate the $RR$ from case-control data**, but we can estimate a related quantity
called the odds-ratio $\ldots$

## The Odds Ratio

* An alternative to the relative risk is the _odds ratio_ $OR$
* Let $p_1=P(\mbox{disease} \mid \mbox{exposed})$ and $p_0=P(\mbox{disease} \mid \mbox{unexposed})$.
* The odds of disease in the exposed group is $p_1/(1-p_1)$ 
* The odds of disease in the unexposed group is $p_0/(1-p_0)$ 
* The odds ratio is 
$$OR = \frac{p_1/(1-p_1)}{p_0/(1-p_0)}$$ 
* Compare to  $RR=p_1/p_0$.

## Connections between $RR$ and $OR$

* If exposure does not affect disease risk, then  $p_0=p_1$, which 
implies that both $RR=1$ and $OR=1$.
* When the disease is rare, both $p_0$ and $p_1$ will be close to zero, so that $1-p_0$ and $1-p_1$ will be close to one. Then the
 $OR$ is approximately equal to the $RR$. 
* When can the $OR$ be used to approximate the $RR$? How rare does the disease have to be? No consensus. In this class we will use a disease prevalence of less than 5\%.

## More on the Odds Ratio 

* After some algebra and applications of Bayes' theorem, can show that the OR is also the ratio of the odds of exposure in the 
 disease group to the odds of exposure in the no-disease group: 
\begin{eqnarray*} OR &=& \frac{p_1/(1-p_1)}{p_0/(1-p_0)} \\
 & = & \frac{P(\mbox{disease}\mid \mbox{exposed})/P(\mbox{no disease} \mid \mbox{exposed})}{P(\mbox{disease}\mid \mbox{unexposed})/P(\mbox{no disease} \mid
\mbox{unexposed})} \\
 & = & \ldots \mbox{algebra} \ldots \\
 & = & \frac{P(\mbox{exposed}\mid \mbox{disease})/P(\mbox{unexposed} \mid \mbox{disease})}{P(\mbox{exposed}\mid \mbox{no disease})/P(\mbox{unexposed} \mid
\mbox{no disease})} \\
 \end{eqnarray*}
  

## More on the Odds Ratio, cont. 

* In other words,
\footnotesize
\begin{eqnarray*}
 OR & = & \frac{\mbox{odds of disease in exposed}}{\mbox{odds of disease in
unexposed}} \\
& = & 
\frac{\mbox{odds of exposure in diseased}}{\mbox{odds of exposure in
not diseased}} 
\end{eqnarray*}
\normalsize

* Importantly, the odds of exposure in either cases or controls ***can***
be estimated from case-control data.
* For example, with Doll and Hill's data,
    + $P(\mbox{exposed} \mid \mbox{not diseased})$ is
estimated by the proportion of smokers in the control group, or
1296/1357, and 
    + $P(\mbox{exposed} \mid \mbox{diseased})$ is
    estimated by the proportion of smokers in the case group, or
    1350/1357
    + Together, these lead to an estimated odds ratio of 
    $$\frac{\frac{1350}{1357}/\frac{7}{1357}}
    {\frac{1296}{1357}/\frac{61}{1357}} = 
    \frac{1350/7}{1296/61}.
    $$
* Hence, we **can estimate the $OR$ from case-control data**.

## Example OR  Estimates 

* Doll and Hill's lung cancer study gave an estimated $OR$ for smoking of 9.1.
* The BC Cancer Agency NHL study gave an estimated $OR$ for 
OGC of 2.7.
* Both lung cancer and NHL are relatively rare, so we may interpret
these estimates as relative risks:
    + smoking increases your risk of lung 
    cancer nine-fold
    + exposure to certain pesticides increases
    your risk of NHL almost three-fold
* We will attach measures of uncertainty to such estimates
(i.e., confidence intervals) in Chapter 15.


## Chapter Summary

* Discussed the basic definitions and rules
of probability, including the definition of conditional probability.
* Use Bayes' Theorem to relate $P(A\mid B)$
to $P(B\mid A)$,$P(A)$ and $P(B)$.
* Public-health and medical 
practitioners work with many conditional probabilities every day; e.g.,
    + diagnostic test sensitivity and specificity
    + relative risks and odds ratios
* Case-control studies oversample cases relative to their frequency
in the population.
    + As a result, we cannot estimate relative risks of disease from 
case-control data. 
    + We can however estimate odds-ratios for disease because
the relative odds of exposure is estimable and equal to the relative
odds of disease.