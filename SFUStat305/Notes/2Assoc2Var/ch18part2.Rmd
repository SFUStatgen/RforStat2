---
title: 'Statistics 305/605: Introduction to Biostatistical Methods for Health Sciences'
subtitle: 'Chapter 18, part 2: Inference in Simple Linear Regression'
author: "Jinko Graham"
output: 
  beamer_presentation:
    includes:
      in_header: ../header_pagenum.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
```


## Inference in Regression

<!--
* Parameters $\alpha$ and $\beta$ 
estimated by $\hat{\alpha}$ 
and $\hat{\beta}$.
-->
* Estimate the population conditional means
    $\mu_{y|x}= \alpha + \beta x$ by $$\hat{\mu}_{y|x}= \hat{y} = \hat{\alpha} + \hat{\beta} x.$$
* If we could observe the errors, 
$\epsilon = Y-\mu_{y|x}$, we could 
estimate $\sigma_{y|x}$ by 
$$ \sqrt{ \frac{1}{n}\sum_{i=1}^n \epsilon_i^2}$$
* But we can't observe the errors because we don't know the population conditional means $\mu_{y|x}$. 
* Instead, substitute the _residuals_, $e=y-\hat{\mu}_{y|x}=y-\hat{y}$, and estimate $\sigma_{y|x}$ by:
$$ s_{y|x} = \sqrt{ \frac{1}{n-2} \sum_{i=1}^{n} e^2_i} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n (y_i-\hat{y}_i)^2 }. $$
Divide by $n-2$, the sample size less the number of parameters used to estimate 
the conditional mean.


## Hypothesis Test for $\beta$

* We can test the null hypothesis of no association between $X$ and $Y$ _vs._
the alternative of association; i.e.,
$$H_0: \beta=0 \;\; \mbox{vs.} \;\;  H_a: \beta \not= 0.$$

* The test statistic is derived from
the sampling distribution of $\hat{\beta}$.

* Assuming that the error terms, $\epsilon$, in the regression model are normally distributed, the sampling distribution of
$\hat{\beta}$ is normal with 
mean $\beta$ and SD
$$
SD(\hat{\beta}) = \frac{\sigma_{y|x}}{\sqrt{ \sum_{i=1}^n (x_i - \bar{x})^2}}
$$
* Replace $\sigma_{y|x}$ by $s_{y|x}$ to get standard error of $\hat{\beta}$, $SE(\hat{\beta})$.


## 

\bigskip

* Replace $\sigma_{y|x}$ by $s_{y|x}$ to get standard error of $\hat{\beta}$:

$$SE(\hat{\beta}) = \frac{s_{y|x}}{\sqrt{ \sum_{i=1}^n (x_i - \bar{x})^2}}
$$.

* We will always use computer software to get
    the SE.
* The pivotal quantity
$$
\frac{\hat{\beta} - \beta}{SE(\hat{\beta})}
$$
has a $t$-distribution with $n-2$ df.
* To test $H_0: \beta=0$ _vs._
$H_a:\beta \not= 0$, 
the test statistic is 
$$T = 
\frac{\hat{\beta} - 0}{SE(\hat{\beta})}
=
\frac{\hat{\beta}}{SE(\hat{\beta})}$$


## Testing Example

* For the low-birthweight babies, let $X$ be the gestational age (in weeks) and
$Y$ be the head circumference (in cm).

* The regression coefficient $\beta$ summarizes the association between
$X$ and $Y$. Test for association using hypotheses 
$H_0: \beta=0$ _vs._ $H_a:\beta \not= 0$


\scriptsize

```{r,echo=FALSE}
uu <- url("http://people.stat.sfu.ca/~jgraham/Teaching/S305_17/Data/lbwt.csv")
lbwt <- read.csv(uu)
lfit <- lm(headcirc ~ gestage,data=lbwt)
summary(lfit)$coefficients
```

\normalsize 

* The test statistic value is about 12.37 and the p-value is tiny.
* There is statistical evidence that gestational age and
head circumference are associated.


## Confidence Intervals

* Following the typical development, a CI for $\beta$ can be derived from the 
pivotal quantity $$\frac{\hat{\beta}-\beta}{SE(\hat{\beta})}$$.
* The level-$C$ CI is of the usual form $$\mbox{estimate} \; \pm \; \mbox{margin of error},$$
where
    + the estimate is $\hat{\beta}$,
    + the margin of error is 
    $SE(\hat{\beta})$ times a critical value
    $t^*$, the upper $(1-C)/2$ critical
    value from the $t$-distribution with $n-2$ df.
    
## Confidence Interval Example

\scriptsize

```{r, echo=FALSE}
confint(lfit,conf.level=0.95)
```

\normalsize

* From the above R output, the 95% CI for $\beta$ is about $(0.65,0.91)$; i.e., in 95 out of 100 samples, we expect the CI to cover the true $\beta$

* One way to interpret (from text):
    + "With 95% confidence, we estimate that a
  one-week increase in gestational age
  is associated with an increase in head circumference
  of between 0.65 to 0.91 cm."
    
## Inference about the Regression Line

* The conditional mean,
$\mu_{y|x}=\alpha + \beta x$, is a population parameter. 

* The fitted value at $x$,
$\hat{y} = \hat{\alpha} + \hat{\beta} x$, 
is an estimate of $\mu_{y|x}$
* The statistic $\hat{y}$ has a sampling distribution whose SD can be estimated by  $SE(\hat{y})$, the standard error
given on page 429 of the text (text's notation is $\widehat{se}(\hat{y})$).
* We can construct a level-$C$ CI for $\mu_{y|x}$ of the usual
form estimate $\pm$ margin of error, where
    + the estimate is $\hat{y}$, and
    + the margin of error is $SE(\hat{y})$ times
    $t^*$,  the upper $(1-C)/2$-critical value of the $t$-distribution
with $n-2$ df.
* We will use a computer to calculate CIs
for the regression line.

## CIs at Observed Values of Explanatory Variable


\scriptsize

```{r, echo=FALSE}
lpred <- predict(lfit,interval="confidence")
lbwtFits <- data.frame(headcirc=lbwt$headcirc,gestage=lbwt$gestage,lpred)
head(lbwtFits)
```

\normalsize
In the R output above:

* The values $y$ of the response variable
are in the column `headcirc`.

* The values $x$ of the explanatory variable are in the column `gestage`.

* The fitted values $\hat{y}$ of the response variable from the regression model are in the column `fit`.

* The lower limits of the CIs for $\mu_{y|x}$ are in the column `lwr` and the upper limits are in the column `upr`.
  
## CIs at New Values of Explanatory Variable

* The output below gives 90% CIs at **new** values of the explanatory variable; i.e., `gestage` of 25.5 and 30.5 weeks.

\scriptsize

```{r,echo=FALSE}
newdat <- data.frame(gestage = c(25.5,30.5))
newpred<-predict(lfit,newdata = newdat, interval="confidence", level=.90)
newFits <- data.frame(gestage=newdat$gestage,newpred)
newFits
```

\normalsize

* The fitted values $\hat{y}$ of `headcirc` for 
`gestage`s of 25.5 and 30.5 are in the column `fit` and are about 23.8 and 27.7, respectively.

* The lower limits of the 90% CIs are in the column `lwr`
and the upper limits are in the column `upr`.

