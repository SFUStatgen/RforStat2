---
title: 'Statistics 305/605: Introduction to Biostatistical Methods for Health Sciences'
subtitle: 'Chapter 18, part 1 (including Demo): Simple Linear-Regression Models'
author: "Jinko Graham"
output: 
  beamer_presentation:
    includes:
      in_header: ../header_pagenum.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
```

## Response and Explanatory Variables

* In simple linear regression, 
    + The **response** variable, $Y$, measures the outcome.   
    +  The **explanatory** variable(s), $X$, are there to explain the outcome.

## Example

* Recall the study of head circumference in 
100 infants with birth weight less than 1500g.
    + Variables included head circumference (cm) and
    gestational age (weeks), among others.
    
\scriptsize
```{r}
uu <- url("http://people.stat.sfu.ca/~jgraham/Teaching/S305_17/Data/lbwt.csv")
lbwt <- read.csv(uu)
head(lbwt)
```
    
\normalsize
    
* Let's view **head circumference** (```headcirc```) as the response variable, $Y$,
with observed measurements denoted $y$.
* Use **gestational age** (```gestage```) as an explanatory variable, $X$, with
observed values $x$. 

## Scatterplot of the Low Birthweight Data


* There appears to be a linear relationship between $Y$ and $X$: 

\scriptsize

```{r,fig.height=2,fig.width=4}
library(ggplot2)
ggplot(lbwt,aes(x=gestage,y=headcirc)) + geom_point()
```


## Linear Regression

* If we have response and explanatory variables, we may
summarize a linear relationship by a **regression
line** through the scatterplot.
* The regression line describes how the average value of $Y$ changes
as $X$ changes.
    + Specifically, the line models the **population mean** of $Y$
given that $X=x$.
* We use the method of least squares to fit or estimate the line from our sample of data.
* Under modelling assumptions, we can:
    + infer
the slope of the regression line in the population from the slope fitted in our sample, and 
    + make predictions from 
the model we have fitted to our data.
* Model assumptions are checked _after_ the model is fit to our sample of data.

## Model Overview

\vspace*{-.05in}

* The components of the statistical model are: 
    1. the linear predictor,
    2. normal error terms, 
    3. constant SD.

\vspace*{-.1in}

\begin{center}
\includegraphics[height=2.1in]{linmodel.jpg}
\end{center}
\vspace*{-.15in}

* Will discuss each component.
* In addition, we assume that the observations are **independent**.

## Linear Predictor

* When there is a linear relationship between $Y$ and $X$,
the conditional mean of $Y$ given $X=x$ in the population, 
denoted $\mu_{y|x}$, is modelled by a \underline{line}:
$$\mu_{y|x} =\alpha + \beta x,$$
* Think of $\mu_{y|x}$ as the population mean value of $Y$ for all data with $X=x$.
* $\beta$ is the change in $\mu_{y|x}$ for a one-unit increase in $x$.



## Normal Errors, Constant SD

* Observed values of $y$ will not fall perfectly along a line.
* Deviations of the $y$'s from the line are called errors.
* Write $y=\alpha + \beta x + \epsilon$ where $\epsilon$ is the 
error term.
* Errors are assumed to be normally distributed with mean zero
and SD $\sigma_{y|x}$.
* The SD of the error terms is assumed to be constant for all $x$; i.e. $\sigma_{y|x}=\sigma_{y}$

## Model Summary

* We can summarize the model assumptions by saying that:
    1. the $(X,Y)$ pairs are independent; 
        + i.e., for individual $i$ with measurements $(X_i,Y_i)$
    and a different individual $j$ with measurements $(X_j,Y_j)$, knowing $i$'s measurements tells us
    nothing about what $j$'s are, and vice versa.
    2. conditional on $X=x$, the outcome $Y$ has a normal distribution $N(\mu_{y|x},\sigma_{y|x})$, with 
        + mean $\mu_{y|x} = \alpha + \beta x$, and       
        + SD $\sigma_{y|x}$ being the same for all $x$, so that $\sigma_{y|x}=\sigma_y$.

## Fitting the Model

* Goal: Let's use the observed data on the $n$ individuals ---  $(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)$ --- to
fit the model
$$\hat{y}_i=\hat{\alpha} + \hat{\beta} x_i,$$
where $\hat{y}_i$ is the **predicted** or **fitted value** of $Y$ for $X=x_i$.

\smallskip

* Idea: Try all possible $\hat{\alpha}$ and $\hat{\beta},$ until we find the line that fits the
data the ``best'' in the sense that
the $\hat{y}$'s are as 
close to the $y$'s as possible. 

\smallskip

* Need to explore the criteria for "best" $\ldots$

## Vertical Distance

* Here is a plot of the data from the low-birth-weight
babies study:
```{r, fig.height=2.5,fig.width=4, echo=FALSE}
ggplot(lbwt,aes(x=gestage,y=headcirc)) + geom_point() + 
  geom_smooth(method="lm",se=FALSE)
```
* By comparing $y$ to $\hat{y}$, we are
measuring the vertical distance between points in 
the scatterplot and the regression line.


## Vertical Distance

* Question: How should we summarize vertical distances between the points, $y$, and the regression line, $\hat{y}$?
* We will discuss the method that minimizes the sum of squared distances,
or least squares.
*  There are many visual demonstrations of the least
squares idea on the internet; e.g., 
\url{http://www.dangoldstein.com/regression.html}
    + The sum of squared distances between the $y$'s and
    their $\hat{y}$'s is summarized by the blue square
    in this demo.
    + To minimize the sum of squared distances, try clicking the buttons for 
         + $-$ slope, $+$ slope, 
         + $-$ intercept, $+$ intercept.
    + Then click "Fit and lock" to see the line 
    that minimizes the sum of squares.


## Least-Squares Regression

* We choose the regression line to minimize the squares of the discrepancies
$y-\hat{y}$; i.e, to
$$ \mbox{minimize} \quad Q = \sum_{i=1}^n(y_i-\hat{y}_i)^2.$$
* The line that minimizes $Q$ has
\begin{eqnarray*}
\hat\beta & = & r \, \frac{s_y}{s_x} \\
\hat\alpha & = & \overline{y} - \hat\beta \, \overline{x},
\end{eqnarray*}
where $r, s_y, s_x, \overline{y}$ and $\overline{x}$ are, respectively: 
    + the sample correlation, the sample SD of $y$, the sample SD of $x$, the sample mean of $y$ and the sample mean of $x$.
* However, we'll use computer software to get
the least-squares estimates of the parameters $\alpha$ and $\beta$.

## Example

* We can superpose the least-squares regression line onto our initial scatterplot of 
head circumference vs. gestational age,
as follows:

\scriptsize

```{r, fig.height=2,fig.width=4}
ggplot(lbwt,aes(x=gestage,y=headcirc)) + geom_point() + 
  geom_smooth(method="lm")
```


## Software Notes

* overlaying `geom_smooth()` adds a
curve to the plot that summarizes the trends and is called a _scatterplot smoother_ 
    + the argument `method=lm` specifies that the smoother should be the 
    least squares regression line.
* The grey shaded region around the regression line is a _point-wise confidence interval_ for
the population means $\mu_{y|x}$: more on these later.

## Fitted Model and Coefficients

* To fit the model in R, we will use the `lm()` function and put the resulting fitted-model into an R object called `lfit`.

\scriptsize

```{r}
lfit <- lm(headcirc ~ gestage,data=lbwt)
names(lfit)
```

\normalsize 

* Let's see what the fitted coefficients are that
estimate the population intercept $\alpha$ and
the population slope $\beta$.

\scriptsize
```{r}
coefficients(lfit)
```

\normalsize

* The estimated intercept and slope are 
$\hat{\alpha} = 3.9$ and $\hat{\beta} = 0.78$.

    + A one week increase in gestational age is associated with an estimated 0.78cm increase in head circumference.


## Software Notes

* `lm()` is the R function that
fits linear models to data by the least-squares method of minimizing the sum of squared vertical distances between the $y$'s and their $\hat{y}$'s.
* `lm()` uses formulas to specify the response and explanatory variables.
    + e.g., in the call to `lm()`,
    we specify   
    `lfit <- lm(headcirc ~ gestage,data=lbwt)`    
    and the formula being
    used is
    `headcirc ~ gestage`
    + the response variable, `headcirc` is on the left-hand side of the formula, to the left of `~`. 
    + the explanatory variable, `gestage` is on the right-hand side of the formula, to
    the right of `~`.
* Extract the fitted coefficients with the `coefficients()` function; i.e.

\scriptsize

```{r}
coefficients(lfit)
```
